{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 229781 entries, 0 to 253679\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Diabetes_012          229781 non-null  int64  \n",
      " 1   HighBP                229781 non-null  float64\n",
      " 2   HighChol              229781 non-null  float64\n",
      " 3   CholCheck             229781 non-null  float64\n",
      " 4   BMI                   229781 non-null  float64\n",
      " 5   Smoker                229781 non-null  float64\n",
      " 6   Stroke                229781 non-null  float64\n",
      " 7   HeartDiseaseorAttack  229781 non-null  float64\n",
      " 8   PhysActivity          229781 non-null  float64\n",
      " 9   Fruits                229781 non-null  float64\n",
      " 10  Veggies               229781 non-null  float64\n",
      " 11  HvyAlcoholConsump     229781 non-null  float64\n",
      " 12  AnyHealthcare         229781 non-null  float64\n",
      " 13  NoDocbcCost           229781 non-null  float64\n",
      " 14  GenHlth               229781 non-null  int64  \n",
      " 15  MentHlth              229781 non-null  float64\n",
      " 16  PhysHlth              229781 non-null  float64\n",
      " 17  DiffWalk              229781 non-null  float64\n",
      " 18  Sex                   229781 non-null  float64\n",
      " 19  Age                   229781 non-null  int64  \n",
      " 20  Education             229781 non-null  int64  \n",
      " 21  Income                229781 non-null  int64  \n",
      "dtypes: float64(17), int64(5)\n",
      "memory usage: 40.3 MB\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## read the raw data file\n",
    "df = pd.read_csv('raw_data/diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "df.head()\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# convert data types\n",
    "df[['Diabetes_012','GenHlth', 'Age', 'Education','Income']] = df[['Diabetes_012','GenHlth', 'Age', 'Education','Income']].astype(int)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229781, 21)\n",
      "(229781,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>Education_4</th>\n",
       "      <th>Education_5</th>\n",
       "      <th>Education_6</th>\n",
       "      <th>Income_2</th>\n",
       "      <th>Income_3</th>\n",
       "      <th>Income_4</th>\n",
       "      <th>Income_5</th>\n",
       "      <th>Income_6</th>\n",
       "      <th>Income_7</th>\n",
       "      <th>Income_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253675</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229781 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0          1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1          0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2          1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3          1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4          1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "...        ...       ...        ...   ...     ...     ...   \n",
       "253675     1.0       1.0        1.0  45.0     0.0     0.0   \n",
       "253676     1.0       1.0        1.0  18.0     0.0     0.0   \n",
       "253677     0.0       0.0        1.0  28.0     0.0     0.0   \n",
       "253678     1.0       0.0        1.0  23.0     0.0     0.0   \n",
       "253679     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  Education_4  \\\n",
       "0                        0.0           0.0     0.0      1.0  ...            1   \n",
       "1                        0.0           1.0     0.0      0.0  ...            0   \n",
       "2                        0.0           0.0     1.0      0.0  ...            1   \n",
       "3                        0.0           1.0     1.0      1.0  ...            0   \n",
       "4                        0.0           1.0     1.0      1.0  ...            0   \n",
       "...                      ...           ...     ...      ...  ...          ...   \n",
       "253675                   0.0           0.0     1.0      1.0  ...            0   \n",
       "253676                   0.0           0.0     0.0      0.0  ...            0   \n",
       "253677                   0.0           1.0     1.0      0.0  ...            0   \n",
       "253678                   0.0           0.0     1.0      1.0  ...            0   \n",
       "253679                   1.0           1.0     1.0      0.0  ...            0   \n",
       "\n",
       "        Education_5  Education_6  Income_2  Income_3  Income_4  Income_5  \\\n",
       "0                 0            0         0         1         0         0   \n",
       "1                 0            1         0         0         0         0   \n",
       "2                 0            0         0         0         0         0   \n",
       "3                 0            0         0         0         0         0   \n",
       "4                 1            0         0         0         1         0   \n",
       "...             ...          ...       ...       ...       ...       ...   \n",
       "253675            0            1         0         0         0         0   \n",
       "253676            0            0         0         0         1         0   \n",
       "253677            1            0         1         0         0         0   \n",
       "253678            1            0         0         0         0         0   \n",
       "253679            0            1         1         0         0         0   \n",
       "\n",
       "        Income_6  Income_7  Income_8  \n",
       "0              0         0         0  \n",
       "1              0         0         0  \n",
       "2              0         0         1  \n",
       "3              1         0         0  \n",
       "4              0         0         0  \n",
       "...          ...       ...       ...  \n",
       "253675         0         1         0  \n",
       "253676         0         0         0  \n",
       "253677         0         0         0  \n",
       "253678         0         0         0  \n",
       "253679         0         0         0  \n",
       "\n",
       "[229781 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slice the dataframe for feature and label\n",
    "df_copy = df.copy(deep=True)\n",
    "X, y = df_copy.iloc[:,1:], df_copy.iloc[:,0]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#get dummies\n",
    "def get_dummies(X):\n",
    "\n",
    "    # Identify discrete columns with more than 2 categories\n",
    "    discrete_columns = [col for col in X.columns if X[col].nunique() > 2 and X[col].dtype != 'float64']\n",
    "\n",
    "    # Create dummy variables for these columns\n",
    "    df_with_dummies = pd.get_dummies(X, columns=discrete_columns, drop_first=True)\n",
    "\n",
    "    return df_with_dummies\n",
    "X = get_dummies(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0]\n",
      "accuracy : 0.8288055008540686  precision : 0.2762685002846895 \n",
      " recall : 0.3333333333333333  f1 : 0.3021299970452513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## This is the baseline dummy classifier which classifies all as the majority class\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "y_dummy_prediction = dummy_majority.predict(X_test)\n",
    "\n",
    "## calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_dummy_prediction)\n",
    "precision = precision_score(y_test, y_dummy_prediction, average='macro')\n",
    "recall = recall_score(y_test, y_dummy_prediction, average='macro')\n",
    "f1 = f1_score(y_test, y_dummy_prediction, average='macro')\n",
    "print('Unique predicted labels: ', (np.unique(y_dummy_prediction)))\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X_train)\n",
    "X_scaled_test = scalar.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8346262226235679  precision : 0.4635717254386112 \n",
      " recall : 0.3840520981044238  f1 : 0.39179133335513555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Moving forward with l2 logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training the logistic regression model\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "\n",
    "\n",
    "# cross_score = cross_val_score(clf, X, y, cv=5, scoring = 'recall_weighted')\n",
    "# print(cross_score)\n",
    "# print(cross_score.mean())\n",
    "\n",
    "clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = clf.predict(X_scaled_test)\n",
    "y_pred_proba = clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# # ROC Curve and AUC\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plotting the ROC Curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8121049253098038  precision : 0.41812355687766023 \n",
      " recall : 0.37650879017541966  f1 : 0.38054216202005287\n"
     ]
    }
   ],
   "source": [
    "## we can see that the baseline model logistic regression perform poorly\n",
    "## try KNN classifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Setting up the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "knn.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_scaled_test)\n",
    "y_pred_proba = knn.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.7432354509155397  precision : 0.38749262989544647 \n",
      " recall : 0.39078354301031065  f1 : 0.3888740388945431\n"
     ]
    }
   ],
   "source": [
    "## we can see that both logistic regression and KNN perform very poorly, lets try more models\n",
    "\n",
    "## Decision trees\n",
    "from sklearn import tree\n",
    "\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = tree_clf.predict(X_scaled_test)\n",
    "y_pred_proba = tree_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear support vector classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = svc_clf.predict(X_scaled_test)\n",
    "y_pred_proba = svc_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.6856919043008062  precision : 0.4150824984684313 \n",
      " recall : 0.46973873690335055  f1 : 0.4093965359532808\n"
     ]
    }
   ],
   "source": [
    "## implement the naive bayes algorithm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_clf = GaussianNB()\n",
    "NB_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = NB_clf.predict(X_scaled_test)\n",
    "y_pred_proba = NB_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8253239476461436  precision : 0.43583333204493885 \n",
      " recall : 0.3779996964043513  f1 : 0.3833000824923048\n"
     ]
    }
   ],
   "source": [
    "## random forest prediction\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)\n",
    "rf_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf_clf.predict(X_scaled_test)\n",
    "y_pred_proba = rf_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8360188439067379  precision : 0.46742952966280865 \n",
      " recall : 0.3886469338780578  f1 : 0.39794023347873436\n"
     ]
    }
   ],
   "source": [
    "## gradient boosing decision trees (GBDT)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GBDT_clf = GradientBoostingClassifier(n_estimators=300)\n",
    "GBDT_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = GBDT_clf.predict(X_scaled_test)\n",
    "y_pred_proba = GBDT_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8356380490246211  precision : 0.46529169192354747 \n",
      " recall : 0.3909616369617985  f1 : 0.4007462396639669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=300)\n",
    "ada_clf.fit(X_scaled, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = ada_clf.predict(X_scaled_test)\n",
    "y_pred_proba = ada_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(45, 64)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        # Third hidden layer\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation function after each hidden layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Apply softmax to the output layer\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming X and y are your features and labels respectively\n",
    "# Convert your data to PyTorch tensors if they are not already\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 183824  # You can adjust this according to your needs\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming X and y are your features and labels respectively\n",
    "# Convert your data to PyTorch tensors if they are not already\n",
    "\n",
    "X_tensor_test = torch.tensor(X_scaled_test, dtype=torch.float32)\n",
    "y_tensor_test = torch.tensor(y_test.to_numpy(), dtype=torch.int64)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.0351, Macro Recall: 0.3333\n",
      "Epoch 2/500, Loss: 0.9793, Macro Recall: 0.3333\n",
      "Epoch 3/500, Loss: 0.8992, Macro Recall: 0.3333\n",
      "Epoch 4/500, Loss: 0.8107, Macro Recall: 0.3333\n",
      "Epoch 5/500, Loss: 0.7511, Macro Recall: 0.3333\n",
      "Epoch 6/500, Loss: 0.7298, Macro Recall: 0.3333\n",
      "Epoch 7/500, Loss: 0.7255, Macro Recall: 0.3333\n",
      "Epoch 8/500, Loss: 0.7250, Macro Recall: 0.3333\n",
      "Epoch 9/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 10/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 11/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 12/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 13/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 14/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 15/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 16/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 17/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 18/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 19/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 20/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 21/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 22/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 23/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 24/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 25/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 26/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 27/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 28/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 29/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 30/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 31/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 32/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 33/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 34/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 35/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 36/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 37/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 38/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 39/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 40/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 41/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 42/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 43/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 44/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 45/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 46/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 47/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 48/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 49/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 50/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 51/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 52/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 53/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 54/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 55/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 56/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 57/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 58/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 59/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 60/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 61/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 62/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 63/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 64/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 65/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 66/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 67/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 68/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 69/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 70/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 71/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 72/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 73/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 74/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 75/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 76/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 77/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 78/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 79/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 80/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 81/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 82/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 83/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 84/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 85/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 86/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 87/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 88/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 89/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 90/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 91/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 92/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 93/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 94/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 95/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 96/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 97/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 98/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 99/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 100/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 101/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 102/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 103/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 104/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 105/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 106/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 107/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 108/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 109/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 110/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 111/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 112/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 113/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 114/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 115/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 116/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 117/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 118/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 119/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 120/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 121/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 122/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 123/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 124/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 125/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 126/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 127/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 128/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 129/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 130/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 131/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 132/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 133/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 134/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 135/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 136/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 137/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 138/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 139/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 140/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 141/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 142/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 143/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 144/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 145/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 146/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 147/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 148/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 149/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 150/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 151/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 152/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 153/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 154/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 155/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 156/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 157/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 158/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 159/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 160/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 161/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 162/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 163/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 164/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 165/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 166/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 167/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 168/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 169/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 170/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 171/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 172/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 173/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 174/500, Loss: 0.7249, Macro Recall: 0.3333\n",
      "Epoch 175/500, Loss: 0.7249, Macro Recall: 0.3333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bruce/Desktop/STATS 302/Group Project/ml_model.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m recall \u001b[39m=\u001b[39m evaluate_model(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/500, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Macro Recall: \u001b[39m\u001b[39m{\u001b[39;00mrecall\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/bruce/Desktop/STATS 302/Group Project/ml_model.ipynb Cell 18\u001b[0m line \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# No need to track gradients for evaluation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/ml_model.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    636\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    678\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    681\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No need to track gradients for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    return recall\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(500):  # num_epochs is the number of times you go through the entire dataset\n",
    "    # all_predictions = []\n",
    "    # all_labels = []\n",
    "    for batch in train_loader:  # data_loader is your PyTorch DataLoader with training data\n",
    "        inputs, labels = batch\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    recall = evaluate_model(model, test_loader)\n",
    "    print(f'Epoch {epoch+1}/500, Loss: {loss.item():.4f}, Macro Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "    #     _, predicted = torch.max(outputs.data, 1)\n",
    "    #     all_predictions.extend(predicted.cpu().numpy())\n",
    "    #     all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # # Calculate recall\n",
    "    # recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    # print(f'Epoch [{epoch+1}/{500}], Loss: {loss.item():.4f}, Recall: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
