{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 229781 entries, 0 to 253679\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Diabetes_012          229781 non-null  int64  \n",
      " 1   HighBP                229781 non-null  float64\n",
      " 2   HighChol              229781 non-null  float64\n",
      " 3   CholCheck             229781 non-null  float64\n",
      " 4   BMI                   229781 non-null  float64\n",
      " 5   Smoker                229781 non-null  float64\n",
      " 6   Stroke                229781 non-null  float64\n",
      " 7   HeartDiseaseorAttack  229781 non-null  float64\n",
      " 8   PhysActivity          229781 non-null  float64\n",
      " 9   Fruits                229781 non-null  float64\n",
      " 10  Veggies               229781 non-null  float64\n",
      " 11  HvyAlcoholConsump     229781 non-null  float64\n",
      " 12  AnyHealthcare         229781 non-null  float64\n",
      " 13  NoDocbcCost           229781 non-null  float64\n",
      " 14  GenHlth               229781 non-null  int64  \n",
      " 15  MentHlth              229781 non-null  float64\n",
      " 16  PhysHlth              229781 non-null  float64\n",
      " 17  DiffWalk              229781 non-null  float64\n",
      " 18  Sex                   229781 non-null  float64\n",
      " 19  Age                   229781 non-null  int64  \n",
      " 20  Education             229781 non-null  int64  \n",
      " 21  Income                229781 non-null  int64  \n",
      "dtypes: float64(17), int64(5)\n",
      "memory usage: 40.3 MB\n",
      "(229781, 21)\n",
      "(229781,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>Education_4</th>\n",
       "      <th>Education_5</th>\n",
       "      <th>Education_6</th>\n",
       "      <th>Income_2</th>\n",
       "      <th>Income_3</th>\n",
       "      <th>Income_4</th>\n",
       "      <th>Income_5</th>\n",
       "      <th>Income_6</th>\n",
       "      <th>Income_7</th>\n",
       "      <th>Income_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253675</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229781 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0          1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1          0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2          1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3          1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4          1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "...        ...       ...        ...   ...     ...     ...   \n",
       "253675     1.0       1.0        1.0  45.0     0.0     0.0   \n",
       "253676     1.0       1.0        1.0  18.0     0.0     0.0   \n",
       "253677     0.0       0.0        1.0  28.0     0.0     0.0   \n",
       "253678     1.0       0.0        1.0  23.0     0.0     0.0   \n",
       "253679     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  Education_4  \\\n",
       "0                        0.0           0.0     0.0      1.0  ...            1   \n",
       "1                        0.0           1.0     0.0      0.0  ...            0   \n",
       "2                        0.0           0.0     1.0      0.0  ...            1   \n",
       "3                        0.0           1.0     1.0      1.0  ...            0   \n",
       "4                        0.0           1.0     1.0      1.0  ...            0   \n",
       "...                      ...           ...     ...      ...  ...          ...   \n",
       "253675                   0.0           0.0     1.0      1.0  ...            0   \n",
       "253676                   0.0           0.0     0.0      0.0  ...            0   \n",
       "253677                   0.0           1.0     1.0      0.0  ...            0   \n",
       "253678                   0.0           0.0     1.0      1.0  ...            0   \n",
       "253679                   1.0           1.0     1.0      0.0  ...            0   \n",
       "\n",
       "        Education_5  Education_6  Income_2  Income_3  Income_4  Income_5  \\\n",
       "0                 0            0         0         1         0         0   \n",
       "1                 0            1         0         0         0         0   \n",
       "2                 0            0         0         0         0         0   \n",
       "3                 0            0         0         0         0         0   \n",
       "4                 1            0         0         0         1         0   \n",
       "...             ...          ...       ...       ...       ...       ...   \n",
       "253675            0            1         0         0         0         0   \n",
       "253676            0            0         0         0         1         0   \n",
       "253677            1            0         1         0         0         0   \n",
       "253678            1            0         0         0         0         0   \n",
       "253679            0            1         1         0         0         0   \n",
       "\n",
       "        Income_6  Income_7  Income_8  \n",
       "0              0         0         0  \n",
       "1              0         0         0  \n",
       "2              0         0         1  \n",
       "3              1         0         0  \n",
       "4              0         0         0  \n",
       "...          ...       ...       ...  \n",
       "253675         0         1         0  \n",
       "253676         0         0         0  \n",
       "253677         0         0         0  \n",
       "253678         0         0         0  \n",
       "253679         0         0         0  \n",
       "\n",
       "[229781 rows x 45 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## read the raw data file\n",
    "df = pd.read_csv('raw_data/diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "df.head()\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# convert data types\n",
    "df[['Diabetes_012','GenHlth', 'Age', 'Education','Income']] = df[['Diabetes_012','GenHlth', 'Age', 'Education','Income']].astype(int)\n",
    "df.info()\n",
    "\n",
    "#slice the dataframe for feature and label\n",
    "df_copy = df.copy(deep=True)\n",
    "X, y = df_copy.iloc[:,1:], df_copy.iloc[:,0]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#get dummies\n",
    "def get_dummies(X):\n",
    "\n",
    "    # Identify discrete columns with more than 2 categories\n",
    "    discrete_columns = [col for col in X.columns if X[col].nunique() > 2 and X[col].dtype != 'float64']\n",
    "\n",
    "    # Create dummy variables for these columns\n",
    "    df_with_dummies = pd.get_dummies(X, columns=discrete_columns, drop_first=True)\n",
    "\n",
    "    return df_with_dummies\n",
    "X = get_dummies(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving forward with l2 logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# define oversampling strategy\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# fit and apply the transform\n",
    "X_over, y_over = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    114033\n",
       "2    114033\n",
       "1    114033\n",
       "Name: Diabetes_012, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## oversample succeed\n",
    "y_over.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X_over)\n",
    "X_scaled_test = scalar.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0]\n",
      "accuracy : 0.8271082436652052  precision : 0.2757027478884017 \n",
      " recall : 0.3333333333333333  f1 : 0.30179136769186493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## This is the baseline dummy classifier which classifies all as the majority class\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_scaled, y_over)\n",
    "y_dummy_prediction = dummy_majority.predict(X_scaled_test)\n",
    "\n",
    "## calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_dummy_prediction)\n",
    "precision = precision_score(y_test, y_dummy_prediction, average='macro')\n",
    "recall = recall_score(y_test, y_dummy_prediction, average='macro')\n",
    "f1 = f1_score(y_test, y_dummy_prediction, average='macro')\n",
    "print('Unique predicted labels: ', (np.unique(y_dummy_prediction)))\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.5120820776168769  precision : 0.427849567900942 \n",
      " recall : 0.4953694647263758  f1 : 0.36834357146141694\n",
      "[[36400 16033 23589]\n",
      " [  268   528  1056]\n",
      " [ 1058  2842 10139]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Moving forward with l2 logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training the logistic regression model\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "\n",
    "clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = clf.predict(X_scaled_test)\n",
    "y_pred_proba = clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bruce/Desktop/STATS 302/Group Project/over_sample.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m knn\u001b[39m.\u001b[39mfit(X_scaled, y_over)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Making predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39;49mpredict(X_scaled_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred_proba \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mpredict_proba(X_scaled_test)[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Calculating metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:214\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    201\u001b[0m     \u001b[39m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m        Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     neigh_dist, neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X)\n\u001b[1;32m    215\u001b[0m     classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    216\u001b[0m     _y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:752\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> 752\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    753\u001b[0m         pairwise_distances_chunked(\n\u001b[1;32m    754\u001b[0m             X,\n\u001b[1;32m    755\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[1;32m    756\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[1;32m    757\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[1;32m    758\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    759\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m    760\u001b[0m         )\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    763\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    764\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1717\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 1717\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1718\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1719\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1721\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1889\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   1887\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 1889\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1430\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1427\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1429\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1430\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1432\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:330\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    325\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m         )\n\u001b[0;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:371\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    368\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    372\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[1;32m    373\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    156\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    159\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## we can see that the baseline model logistic regression perform poorly\n",
    "## try KNN classifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Setting up the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "knn.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_scaled_test)\n",
    "y_pred_proba = knn.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.7365363274365168  precision : 0.38819800948918304 \n",
      " recall : 0.39423082774631113  f1 : 0.39059341247247525\n"
     ]
    }
   ],
   "source": [
    "## we can see that both logistic regression and KNN perform very poorly, lets try more models\n",
    "\n",
    "## Decision trees\n",
    "from sklearn import tree\n",
    "\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = tree_clf.predict(X_scaled_test)\n",
    "y_pred_proba = tree_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear support vector classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = svc_clf.predict(X_scaled_test)\n",
    "y_pred_proba = svc_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.5388297756598559  precision : 0.4087960216518305 \n",
      " recall : 0.4697972837107261  f1 : 0.36087536776708623\n"
     ]
    }
   ],
   "source": [
    "## implement the naive bayes algorithm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_clf = GaussianNB()\n",
    "NB_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = NB_clf.predict(X_scaled_test)\n",
    "y_pred_proba = NB_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.816611180016102  precision : 0.4457947826499265 \n",
      " recall : 0.42451176258655626  f1 : 0.42976350387690515\n"
     ]
    }
   ],
   "source": [
    "## random forest prediction\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)\n",
    "rf_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf_clf.predict(X_scaled_test)\n",
    "y_pred_proba = rf_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.6117240028722501  precision : 0.44217302727755037 \n",
      " recall : 0.5133180217151039  f1 : 0.41728635264479114\n"
     ]
    }
   ],
   "source": [
    "## gradient boosing decision trees (GBDT)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GBDT_clf = GradientBoostingClassifier(n_estimators=300)\n",
    "GBDT_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = GBDT_clf.predict(X_scaled_test)\n",
    "y_pred_proba = GBDT_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.6166198838044259  precision : 0.44123649529048753 \n",
      " recall : 0.51405201132797  f1 : 0.4187735255441163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=300)\n",
    "ada_clf.fit(X_scaled, y_over)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = ada_clf.predict(X_scaled_test)\n",
    "y_pred_proba = ada_clf.predict_proba(X_scaled_test)[:, 1]\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "# cv_scores = cross_val_score(knn, X, y, cv=5)\n",
    "# cv_scores\n",
    "\n",
    "print('accuracy : %s  precision : %s \\n recall : %s  f1 : %s'\n",
    "      % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(45, 64)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        # Third hidden layer\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation function after each hidden layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Apply softmax to the output layer\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming X and y are your features and labels respectively\n",
    "# Convert your data to PyTorch tensors if they are not already\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_over, dtype=torch.int64)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 183824  # You can adjust this according to your needs\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "X_tensor_test = torch.tensor(X_scaled_test, dtype=torch.float32)\n",
    "y_tensor_test = torch.tensor(y_test.to_numpy(), dtype=torch.int64)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bruce/Desktop/STATS 302/Group Project/over_sample.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Example training loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):  \u001b[39m# num_epochs is the number of times you go through the entire dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# all_predictions = []\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# all_labels = []\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:  \u001b[39m# data_loader is your PyTorch DataLoader with training data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39m# inputs = inputs.to('mps:0')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39m# labels = labels.to('mps:0')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bruce/Desktop/STATS%20302/Group%20Project/over_sample.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39m# Zero the parameter gradients\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "model = NeuralNetwork()\n",
    "# model = model.to('mps:0')\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No need to track gradients for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    return recall\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(500):  # num_epochs is the number of times you go through the entire dataset\n",
    "    # all_predictions = []\n",
    "    # all_labels = []\n",
    "    for batch in train_loader:  # data_loader is your PyTorch DataLoader with training data\n",
    "        inputs, labels = batch\n",
    "        # inputs = inputs.to('mps:0')\n",
    "        # labels = labels.to('mps:0')\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    recall = evaluate_model(model, test_loader)\n",
    "    print(f'Epoch {epoch+1}/500, Loss: {loss.item():.4f}, Macro Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "    #     _, predicted = torch.max(outputs.data, 1)\n",
    "    #     all_predictions.extend(predicted.cpu().numpy())\n",
    "    #     all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # # Calculate recall\n",
    "    # recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    # print(f'Epoch [{epoch+1}/{500}], Loss: {loss.item():.4f}, Recall: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
